{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Multi-Model Endpoints using your own algorithm container\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-2/advanced_functionality|multi_model_bring_your_own|multi_model_endpoint_bring_your_own.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With [Amazon SageMaker multi-model endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html), customers can create an endpoint that seamlessly hosts up to thousands of models. These endpoints are well suited to use cases where any one of a large number of models, which can be served from a common inference container, needs to be invokable on-demand and where it is acceptable for infrequently invoked models to incur some additional latency. For applications which require consistently low inference latency, a traditional endpoint is still the best choice.\n",
    "\n",
    "At a high level, Amazon SageMaker manages the loading and unloading of models for a multi-model endpoint, as they are needed. When an invocation request is made for a particular model, Amazon SageMaker routes the request to an instance assigned to that model, downloads the model artifacts from S3 onto that instance, and initiates loading of the model into the memory of the container. As soon as the loading is complete, Amazon SageMaker performs the requested invocation and returns the result. If the model is already loaded in memory on the selected instance, the downloading and loading steps are skipped and the invocation is performed immediately.\n",
    "\n",
    "For the inference container to serve multiple models in a multi-model endpoint, it must implement [additional APIs](https://docs.aws.amazon.com/sagemaker/latest/dg/build-multi-model-build-container.html) in order to load, list, get, unload and invoke specific models. This notebook demonstrates how to build your own inference container that implements these APIs.\n",
    "\n",
    "**Note**: Because this notebook builds a Docker container, it does not run in Amazon SageMaker Studio.\n",
    "\n",
    "This notebook was tested with the `conda_mxnet_p36` kernel running SageMaker Python SDK version 2.15.3 on an Amazon SageMaker notebook instance.\n",
    "\n",
    "---\n",
    "\n",
    "### Contents\n",
    "\n",
    "1. [Introduction to Multi Model Server (MMS)](#Introduction-to-Multi-Model-Server-(MMS))\n",
    "  1. [Handling Out Of Memory conditions](#Handling-Out-Of-Memory-conditions)\n",
    "  1. [SageMaker Inference Toolkit](#SageMaker-Inference-Toolkit)\n",
    "1. [Building and registering a container using MMS](#Building-and-registering-a-container-using-MMS)\n",
    "1. [Set up the environment](#Set-up-the-environment)\n",
    "1. [Upload model artifacts to S3](#Upload-model-artifacts-to-S3)\n",
    "1. [Create a multi-model endpoint](#Create-a-multi-model-endpoint)\n",
    "  1. [Import models into hosting](#Import-models-into-hosting)\n",
    "  1. [Create endpoint configuration](#Create-endpoint-configuration)\n",
    "  1. [Create endpoint](#Create-endpoint)\n",
    "1. [Invoke models](#Invoke-models)\n",
    "  1. [Add models to the endpoint](#Add-models-to-the-endpoint)\n",
    "  1. [Updating a model](#Updating-a-model)\n",
    "1. [(Optional) Delete the hosting resources](#(Optional)-Delete-the-hosting-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Multi Model Server (MMS)\n",
    "\n",
    "[Multi Model Server](https://github.com/awslabs/multi-model-server) is an open source framework for serving machine learning models. It provides the HTTP frontend and model management capabilities required by multi-model endpoints to host multiple models within a single container, load models into and unload models out of the container dynamically, and performing inference on a specified loaded model.\n",
    "\n",
    "MMS supports a pluggable custom backend handler where you can implement your own algorithm. This example uses a handler that supports loading and inference for MXNet models, which we will inspect below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "ModelHandler defines an example model handler for load and inference requests for MXNet CPU models\n",
      "\"\"\"\n",
      "import glob\n",
      "import json\n",
      "import logging\n",
      "import os\n",
      "import re\n",
      "from collections import namedtuple\n",
      "\n",
      "import mxnet as mx\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "class ModelHandler(object):\n",
      "    \"\"\"\n",
      "    A sample Model handler implementation.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        self.initialized = False\n",
      "        self.mx_model = None\n",
      "        self.shapes = None\n",
      "\n",
      "    def get_model_files_prefix(self, model_dir):\n",
      "        \"\"\"\n",
      "        Get the model prefix name for the model artifacts (symbol and parameter file).\n",
      "        This assume model artifact directory contains a symbol file, parameter file,\n",
      "        model shapes file and a synset file defining the labels\n",
      "\n",
      "        :param model_dir: Path to the directory with model artifacts\n",
      "        :return: prefix string for model artifact files\n",
      "        \"\"\"\n",
      "        sym_file_suffix = \"-symbol.json\"\n",
      "        checkpoint_prefix_regex = \"{}/*{}\".format(\n",
      "            model_dir, sym_file_suffix\n",
      "        )  # Ex output: /opt/ml/models/resnet-18/model/*-symbol.json\n",
      "        checkpoint_prefix_filename = glob.glob(checkpoint_prefix_regex)[\n",
      "            0\n",
      "        ]  # Ex output: /opt/ml/models/resnet-18/model/resnet18-symbol.json\n",
      "        checkpoint_prefix = os.path.basename(checkpoint_prefix_filename).split(sym_file_suffix)[\n",
      "            0\n",
      "        ]  # Ex output: resnet18\n",
      "        logging.info(\"Prefix for the model artifacts: {}\".format(checkpoint_prefix))\n",
      "        return checkpoint_prefix\n",
      "\n",
      "    def get_input_data_shapes(self, model_dir, checkpoint_prefix):\n",
      "        \"\"\"\n",
      "        Get the model input data shapes and return the list\n",
      "\n",
      "        :param model_dir: Path to the directory with model artifacts\n",
      "        :param checkpoint_prefix: Model files prefix name\n",
      "        :return: prefix string for model artifact files\n",
      "        \"\"\"\n",
      "        shapes_file_path = os.path.join(model_dir, \"{}-{}\".format(checkpoint_prefix, \"shapes.json\"))\n",
      "        if not os.path.isfile(shapes_file_path):\n",
      "            raise RuntimeError(\"Missing {} file.\".format(shapes_file_path))\n",
      "\n",
      "        with open(shapes_file_path) as f:\n",
      "            self.shapes = json.load(f)\n",
      "\n",
      "        data_shapes = []\n",
      "\n",
      "        for input_data in self.shapes:\n",
      "            data_name = input_data[\"name\"]\n",
      "            data_shape = input_data[\"shape\"]\n",
      "            data_shapes.append((data_name, tuple(data_shape)))\n",
      "\n",
      "        return data_shapes\n",
      "\n",
      "    def initialize(self, context):\n",
      "        \"\"\"\n",
      "        Initialize model. This will be called during model loading time\n",
      "        :param context: Initial context contains model server system properties.\n",
      "        :return:\n",
      "        \"\"\"\n",
      "        self.initialized = True\n",
      "        properties = context.system_properties\n",
      "        # Contains the url parameter passed to the load request\n",
      "        model_dir = properties.get(\"model_dir\")\n",
      "        gpu_id = properties.get(\"gpu_id\")\n",
      "\n",
      "        checkpoint_prefix = self.get_model_files_prefix(model_dir)\n",
      "\n",
      "        # Read the model input data shapes\n",
      "        data_shapes = self.get_input_data_shapes(model_dir, checkpoint_prefix)\n",
      "\n",
      "        # Load MXNet model\n",
      "        try:\n",
      "            ctx = mx.cpu()  # Set the context on CPU\n",
      "            sym, arg_params, aux_params = mx.model.load_checkpoint(\n",
      "                checkpoint_prefix, 0\n",
      "            )  # epoch set to 0\n",
      "            self.mx_model = mx.mod.Module(symbol=sym, context=ctx, label_names=None)\n",
      "            self.mx_model.bind(\n",
      "                for_training=False,\n",
      "                data_shapes=data_shapes,\n",
      "                label_shapes=self.mx_model._label_shapes,\n",
      "            )\n",
      "            self.mx_model.set_params(arg_params, aux_params, allow_missing=True)\n",
      "            with open(\"synset.txt\", \"r\") as f:\n",
      "                self.labels = [l.rstrip() for l in f]\n",
      "        except (mx.base.MXNetError, RuntimeError) as memerr:\n",
      "            if re.search(\"Failed to allocate (.*) Memory\", str(memerr), re.IGNORECASE):\n",
      "                logging.error(\"Memory allocation exception: {}\".format(memerr))\n",
      "                raise MemoryError\n",
      "            raise\n",
      "\n",
      "    def preprocess(self, request):\n",
      "        \"\"\"\n",
      "        Transform raw input into model input data.\n",
      "        :param request: list of raw requests\n",
      "        :return: list of preprocessed model input data\n",
      "        \"\"\"\n",
      "        # Take the input data and pre-process it make it inference ready\n",
      "\n",
      "        img_list = []\n",
      "        for idx, data in enumerate(request):\n",
      "            # Read the bytearray of the image from the input\n",
      "            img_arr = data.get(\"body\")\n",
      "\n",
      "            # Input image is in bytearray, convert it to MXNet NDArray\n",
      "            img = mx.img.imdecode(img_arr)\n",
      "            if img is None:\n",
      "                return None\n",
      "\n",
      "            # convert into format (batch, RGB, width, height)\n",
      "            img = mx.image.imresize(img, 224, 224)  # resize\n",
      "            img = img.transpose((2, 0, 1))  # Channel first\n",
      "            img = img.expand_dims(axis=0)  # batchify\n",
      "            img_list.append(img)\n",
      "\n",
      "        return img_list\n",
      "\n",
      "    def inference(self, model_input):\n",
      "        \"\"\"\n",
      "        Internal inference methods\n",
      "        :param model_input: transformed model input data list\n",
      "        :return: list of inference output in NDArray\n",
      "        \"\"\"\n",
      "        # Do some inference call to engine here and return output\n",
      "        Batch = namedtuple(\"Batch\", [\"data\"])\n",
      "        self.mx_model.forward(Batch(model_input))\n",
      "        prob = self.mx_model.get_outputs()[0].asnumpy()\n",
      "        return prob\n",
      "\n",
      "    def postprocess(self, inference_output):\n",
      "        \"\"\"\n",
      "        Return predict result in as list.\n",
      "        :param inference_output: list of inference output\n",
      "        :return: list of predict results\n",
      "        \"\"\"\n",
      "        # Take output from network and post-process to desired format\n",
      "        prob = np.squeeze(inference_output)\n",
      "        a = np.argsort(prob)[::-1]\n",
      "        return [[\"probability=%f, class=%s\" % (prob[i], self.labels[i]) for i in a[0:5]]]\n",
      "\n",
      "    def handle(self, data, context):\n",
      "        \"\"\"\n",
      "        Call preprocess, inference and post-process functions\n",
      "        :param data: input data\n",
      "        :param context: mms context\n",
      "        \"\"\"\n",
      "\n",
      "        model_input = self.preprocess(data)\n",
      "        model_out = self.inference(model_input)\n",
      "        return self.postprocess(model_out)\n",
      "\n",
      "\n",
      "_service = ModelHandler()\n",
      "\n",
      "\n",
      "def handle(data, context):\n",
      "    if not _service.initialized:\n",
      "        _service.initialize(context)\n",
      "\n",
      "    if data is None:\n",
      "        return None\n",
      "\n",
      "    return _service.handle(data, context)\n"
     ]
    }
   ],
   "source": [
    "!cat container/model_handler.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of note are the `handle(data, context)` and `initialize(self, context)` methods.\n",
    "\n",
    "The `initialize` method will be called when a model is loaded into memory. In this example, it loads the model artifacts at `model_dir` into MXNet.\n",
    "\n",
    "The `handle` method will be called when invoking the model. In this example, it validates the input payload and then forwards the input to MXNet, returning the output.\n",
    "\n",
    "This handler class is instantiated for every model loaded into the container, so state in the handler is not shared across models.\n",
    "\n",
    "### Handling Out Of Memory conditions\n",
    "If MXNet fails to load the model due to lack of memory, a `MemoryError` is raised. Any time a model cannot be loaded due to lack of memory or any other resource constraint, a `MemoryError` must be raised. MMS will interpret the `MemoryError`, and return a 507 HTTP status code to SageMaker, where SageMaker will initiate unloading unused models to reclaim resources so the requested model can be loaded.\n",
    "\n",
    "### SageMaker Inference Toolkit\n",
    "MMS supports [various settings](https://github.com/awslabs/multi-model-server/blob/master/docker/advanced_settings.md#description-of-config-file-settings) for the frontend server it starts.\n",
    "\n",
    "[SageMaker Inference Toolkit](https://github.com/aws/sagemaker-inference-toolkit) is a library that bootstraps MMS in a way that is compatible with SageMaker multi-model endpoints, while still allowing you to tweak important performance parameters, such as the number of workers per model. The inference container in this example uses the Inference Toolkit to start MMS which can be seen in the __`container/dockerd-entrypoint.py`__ file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and registering a container using MMS\n",
    "\n",
    "The shell script below will build a Docker image which uses MMS as the front end (configured through SageMaker Inference Toolkit), and `container/model_handler.py` that we inspected above as the backend handler. It will then upload the image to an ECR repository in your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "sha256:a34bf96e2631def7cb8d990fe9854cccc2a9f4eea7b94217e1f51f1a9a971687\n",
      "The push refers to repository [223890829887.dkr.ecr.us-east-1.amazonaws.com/demo-sagemaker-multimodel]\n",
      "7d64778ade40: Preparing\n",
      "1591a332f235: Preparing\n",
      "9ac9b49533ee: Preparing\n",
      "e152f3c7746d: Preparing\n",
      "bbcc3bbcc1de: Preparing\n",
      "4ca8d07f6f56: Preparing\n",
      "b7710361ee87: Preparing\n",
      "c24f6496c653: Preparing\n",
      "3b5a5314b205: Preparing\n",
      "c4d974e8b286: Preparing\n",
      "59331cfafad6: Preparing\n",
      "62191d14d4c2: Preparing\n",
      "c09969dbc5e8: Preparing\n",
      "c24f6496c653: Waiting\n",
      "3b5a5314b205: Waiting\n",
      "c4d974e8b286: Waiting\n",
      "59331cfafad6: Waiting\n",
      "62191d14d4c2: Waiting\n",
      "c09969dbc5e8: Waiting\n",
      "4ca8d07f6f56: Waiting\n",
      "b7710361ee87: Waiting\n",
      "9ac9b49533ee: Pushed\n",
      "7d64778ade40: Pushed\n",
      "1591a332f235: Pushed\n",
      "e152f3c7746d: Pushed\n",
      "c24f6496c653: Layer already exists\n",
      "3b5a5314b205: Layer already exists\n",
      "b7710361ee87: Layer already exists\n",
      "4ca8d07f6f56: Layer already exists\n",
      "c4d974e8b286: Layer already exists\n",
      "59331cfafad6: Layer already exists\n",
      "c09969dbc5e8: Layer already exists\n",
      "62191d14d4c2: Layer already exists\n",
      "bbcc3bbcc1de: Pushed\n",
      "latest: digest: sha256:2b1205368bffed8f2ecf85bcd69efbdfa5f35bede9962de4ee381def29f4e077 size: 3039\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=demo-sagemaker-multimodel\n",
    "\n",
    "cd container\n",
    "\n",
    "account=223890829887\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "# region=$(aws configure get region)\n",
    "region=us-east-1\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password \\\n",
    "    --region ${region} \\\n",
    "| docker login \\\n",
    "    --username AWS \\\n",
    "    --password-stdin ${account}.dkr.ecr.${region}.amazonaws.com\n",
    "\n",
    "# aws ecr get-login-password --region us-east-1 | docker login \\\n",
    "#     --username AWS \\\n",
    "#     --password-stdin 223890829887.dkr.ecr.us-east-1.amazonaws.com\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -q -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "Define the S3 bucket and prefix where the model artifacts that will be invokable by your multi-model endpoint will be located.\n",
    "\n",
    "Also define the IAM role that will give SageMaker access to the model artifacts and ECR image that was created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.1.5 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.1.5 requires pyqtwebengine<5.13, which is not installed.\n",
      "apache-airflow 2.3.3 requires attrs<21.0,>=20.0, but you have attrs 23.1.0 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU awscli boto3 sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/hwajjala/Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/hwajjala/Library/Application Support/sagemaker/config.yaml\n",
      "arn:aws:iam::223890829887:role/aws-reserved/sso.amazonaws.com/AWSReservedSSO_Poweruser_658f486a37df46ae\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "bucket = \"sagemaker-{}-{}\".format(region, account_id)\n",
    "prefix = \"demo-multimodel-endpoint\"\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload model artifacts to S3\n",
    "In this example we will use pre-trained ResNet 18 and ResNet 152 models, both trained on the ImageNet datset. First we will download the models from MXNet's model zoo, and then upload them to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mxnet as mx\n",
    "# import os\n",
    "# import tarfile\n",
    "\n",
    "# model_path = \"http://data.mxnet.io/models/imagenet/\"\n",
    "\n",
    "# mx.test_utils.download(\n",
    "#     model_path + \"resnet/18-layers/resnet-18-0000.params\", None, \"data/resnet_18\"\n",
    "# )\n",
    "# mx.test_utils.download(\n",
    "#     model_path + \"resnet/18-layers/resnet-18-symbol.json\", None, \"data/resnet_18\"\n",
    "# )\n",
    "# mx.test_utils.download(model_path + \"synset.txt\", None, \"data/resnet_18\")\n",
    "\n",
    "# with open(\"data/resnet_18/resnet-18-shapes.json\", \"w\") as file:\n",
    "#     file.write('[{\"shape\": [1, 3, 224, 224], \"name\": \"data\"}]')\n",
    "\n",
    "# with tarfile.open(\"data/resnet_18.tar.gz\", \"w:gz\") as tar:\n",
    "#     tar.add(\"data/resnet_18\", arcname=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mx.test_utils.download(\n",
    "#     model_path + \"resnet/152-layers/resnet-152-0000.params\", None, \"data/resnet_152\"\n",
    "# )\n",
    "# mx.test_utils.download(\n",
    "#     model_path + \"resnet/152-layers/resnet-152-symbol.json\", None, \"data/resnet_152\"\n",
    "# )\n",
    "# mx.test_utils.download(model_path + \"synset.txt\", None, \"data/resnet_152\")\n",
    "\n",
    "# with open(\"data/resnet_152/resnet-152-shapes.json\", \"w\") as file:\n",
    "#     file.write('[{\"shape\": [1, 3, 224, 224], \"name\": \"data\"}]')\n",
    "\n",
    "# with tarfile.open(\"data/resnet_152.tar.gz\", \"w:gz\") as tar:\n",
    "#     tar.add(\"data/resnet_152\", arcname=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.client import ClientError\n",
    "import os\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "try:\n",
    "    s3.meta.client.head_bucket(Bucket=bucket)\n",
    "except ClientError:\n",
    "    s3.create_bucket(Bucket=bucket, CreateBucketConfiguration={\"LocationConstraint\": region})\n",
    "\n",
    "models = {\"resnet_18.tar.gz\", \"resnet_152.tar.gz\"}\n",
    "\n",
    "for model in models:\n",
    "    key = os.path.join(prefix, model)\n",
    "    with open(\"data/\" + model, \"rb\") as file_obj:\n",
    "        s3.Bucket(bucket).Object(key).upload_fileobj(file_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a multi-model endpoint\n",
    "### Import models into hosting\n",
    "When creating the Model entity for multi-model endpoints, the container's `ModelDataUrl` is the S3 prefix where the model artifacts that are invokable by the endpoint are located. The rest of the S3 path will be specified when invoking the model.\n",
    "\n",
    "The `Mode` of container is specified as `MultiModel` to signify that the container will host multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: DEMO-MultiModelModel2023-10-07-14-40-07\n",
      "Model data Url: https://s3-us-east-1.amazonaws.com/sagemaker-us-east-1-223890829887/demo-multimodel-endpoint/\n",
      "Container image: 223890829887.dkr.ecr.us-east-1.amazonaws.com/demo-sagemaker-multimodel:latest\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateModel operation: The execution role ARN \"arn:aws:iam::223890829887:role/aws-reserved/sso.amazonaws.com/AWSReservedSSO_Poweruser_658f486a37df46ae\" is invalid. Please ensure that the role exists and that its trust relationship policy allows the action \"sts:AssumeRole\" for the service principal \"sagemaker.amazonaws.com\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/hwajjala/workspace/amazon-sagemaker-examples/advanced_functionality/multi_model_bring_your_own/multi_model_endpoint_bring_your_own.ipynb Cell 17\u001b[0m line \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hwajjala/workspace/amazon-sagemaker-examples/advanced_functionality/multi_model_bring_your_own/multi_model_endpoint_bring_your_own.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mContainer image: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m container)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hwajjala/workspace/amazon-sagemaker-examples/advanced_functionality/multi_model_bring_your_own/multi_model_endpoint_bring_your_own.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m container \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mImage\u001b[39m\u001b[39m\"\u001b[39m: container, \u001b[39m\"\u001b[39m\u001b[39mModelDataUrl\u001b[39m\u001b[39m\"\u001b[39m: model_url, \u001b[39m\"\u001b[39m\u001b[39mMode\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mMultiModel\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hwajjala/workspace/amazon-sagemaker-examples/advanced_functionality/multi_model_bring_your_own/multi_model_endpoint_bring_your_own.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m create_model_response \u001b[39m=\u001b[39m sm_client\u001b[39m.\u001b[39;49mcreate_model(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hwajjala/workspace/amazon-sagemaker-examples/advanced_functionality/multi_model_bring_your_own/multi_model_endpoint_bring_your_own.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     ModelName\u001b[39m=\u001b[39;49mmodel_name, ExecutionRoleArn\u001b[39m=\u001b[39;49mrole, Containers\u001b[39m=\u001b[39;49m[container]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hwajjala/workspace/amazon-sagemaker-examples/advanced_functionality/multi_model_bring_your_own/multi_model_endpoint_bring_your_own.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hwajjala/workspace/amazon-sagemaker-examples/advanced_functionality/multi_model_bring_your_own/multi_model_endpoint_bring_your_own.ipynb#X22sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel Arn: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m create_model_response[\u001b[39m\"\u001b[39m\u001b[39mModelArn\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vscode/lib/python3.9/site-packages/botocore/client.py:535\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    532\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[1;32m    534\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vscode/lib/python3.9/site-packages/botocore/client.py:980\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    978\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    979\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 980\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    981\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateModel operation: The execution role ARN \"arn:aws:iam::223890829887:role/aws-reserved/sso.amazonaws.com/AWSReservedSSO_Poweruser_658f486a37df46ae\" is invalid. Please ensure that the role exists and that its trust relationship policy allows the action \"sts:AssumeRole\" for the service principal \"sagemaker.amazonaws.com\"."
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "model_name = \"DEMO-MultiModelModel\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "model_url = \"https://s3-{}.amazonaws.com/{}/{}/\".format(region, bucket, prefix)\n",
    "container = \"{}.dkr.ecr.{}.amazonaws.com/{}:latest\".format(\n",
    "    account_id, region, \"demo-sagemaker-multimodel\"\n",
    ")\n",
    "\n",
    "print(\"Model name: \" + model_name)\n",
    "print(\"Model data Url: \" + model_url)\n",
    "print(\"Container image: \" + container)\n",
    "\n",
    "container = {\"Image\": container, \"ModelDataUrl\": model_url, \"Mode\": \"MultiModel\"}\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name, ExecutionRoleArn=role, Containers=[container]\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint configuration\n",
    "Endpoint config creation works the same way it does as single model endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = \"DEMO-MultiModelEndpointConfig-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Endpoint config name: \" + endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.m5.xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            # \"InitialVariantWeight\": 1,\n",
    "            \"ModelName\": model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint\n",
    "Similarly, endpoint creation works the same way as for single model endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "endpoint_name = \"DEMO-MultiModelEndpoint-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Endpoint name: \" + endpoint_name)\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Endpoint Status: \" + status)\n",
    "\n",
    "print(\"Waiting for {} endpoint to be in service...\".format(endpoint_name))\n",
    "waiter = sm_client.get_waiter(\"endpoint_in_service\")\n",
    "waiter.wait(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke models\n",
    "Now we invoke the models that we uploaded to S3 previously. The first invocation of a model may be slow, since behind the scenes, SageMaker is downloading the model artifacts from S3 to the instance and loading it into the container.\n",
    "\n",
    "First we will download an image of a cat as the payload to invoke the model, then call InvokeEndpoint to invoke the ResNet 18 model. The `TargetModel` field is concatenated with the S3 prefix specified in `ModelDataUrl` when creating the model, to generate the location of the model in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = mx.test_utils.download(\n",
    "    \"https://github.com/dmlc/web-data/blob/master/mxnet/doc/tutorials/python/predict_image/cat.jpg?raw=true\",\n",
    "    \"cat.jpg\",\n",
    ")\n",
    "\n",
    "with open(fname, \"rb\") as f:\n",
    "    payload = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/x-image\",\n",
    "    TargetModel=\"resnet_18.tar.gz\",  # this is the rest of the S3 path where the model artifacts are located\n",
    "    Body=payload,\n",
    ")\n",
    "\n",
    "print(*json.loads(response[\"Body\"].read()), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we invoke the same ResNet 18 model a 2nd time, it is already downloaded to the instance and loaded in the container, so inference is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/x-image\",\n",
    "    TargetModel=\"resnet_18.tar.gz\",\n",
    "    Body=payload,\n",
    ")\n",
    "\n",
    "print(*json.loads(response[\"Body\"].read()), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke another model\n",
    "Exercising the power of a multi-model endpoint, we can specify a different model (resnet_152.tar.gz) as `TargetModel` and perform inference on it using the same endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/x-image\",\n",
    "    TargetModel=\"resnet_152.tar.gz\",\n",
    "    Body=payload,\n",
    ")\n",
    "\n",
    "print(*json.loads(response[\"Body\"].read()), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add models to the endpoint\n",
    "We can add more models to the endpoint without having to update the endpoint. Below we are adding a 3rd model, `squeezenet_v1.0`. To demonstrate hosting multiple models behind the endpoint, this model is duplicated 10 times with a slightly different name in S3. In a more realistic scenario, these could be 10 new different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.test_utils.download(\n",
    "    model_path + \"squeezenet/squeezenet_v1.0-0000.params\", None, \"data/squeezenet_v1.0\"\n",
    ")\n",
    "mx.test_utils.download(\n",
    "    model_path + \"squeezenet/squeezenet_v1.0-symbol.json\", None, \"data/squeezenet_v1.0\"\n",
    ")\n",
    "mx.test_utils.download(model_path + \"synset.txt\", None, \"data/squeezenet_v1.0\")\n",
    "\n",
    "with open(\"data/squeezenet_v1.0/squeezenet_v1.0-shapes.json\", \"w\") as file:\n",
    "    file.write('[{\"shape\": [1, 3, 224, 224], \"name\": \"data\"}]')\n",
    "\n",
    "with tarfile.open(\"data/squeezenet_v1.0.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"data/squeezenet_v1.0\", arcname=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"data/squeezenet_v1.0.tar.gz\"\n",
    "\n",
    "for x in range(0, 10):\n",
    "    s3_file_name = \"demo-subfolder/squeezenet_v1.0_{}.tar.gz\".format(x)\n",
    "    key = os.path.join(prefix, s3_file_name)\n",
    "    with open(file, \"rb\") as file_obj:\n",
    "        s3.Bucket(bucket).Object(key).upload_fileobj(file_obj)\n",
    "    models.add(s3_file_name)\n",
    "\n",
    "print(\"Number of models: {}\".format(len(models)))\n",
    "print(\"Models: {}\".format(models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After uploading the SqueezeNet models to S3, we will invoke the endpoint 100 times, randomly choosing from one of the 12 models behind the S3 prefix for each invocation, and keeping a count of the label with the highest probability on each invoke response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "results = defaultdict(int)\n",
    "\n",
    "for x in range(0, 100):\n",
    "    target_model = random.choice(tuple(models))\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/x-image\",\n",
    "        TargetModel=target_model,\n",
    "        Body=payload,\n",
    "    )\n",
    "\n",
    "    results[json.loads(response[\"Body\"].read())[0]] += 1\n",
    "\n",
    "print(*results.items(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating a model\n",
    "To update a model, you would follow the same approach as above and add it as a new model. For example, if you have retrained the `resnet_18.tar.gz` model and wanted to start invoking it, you would upload the updated model artifacts behind the S3 prefix with a new name such as `resnet_18_v2.tar.gz`, and then change the `TargetModel` field to invoke `resnet_18_v2.tar.gz` instead of `resnet_18.tar.gz`. You do not want to overwrite the model artifacts in Amazon S3, because the old version of the model might still be loaded in the containers or on the storage volume of the instances on the endpoint. Invocations to the new model could then invoke the old version of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Delete the hosting resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-1/advanced_functionality|multi_model_bring_your_own|multi_model_endpoint_bring_your_own.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-2/advanced_functionality|multi_model_bring_your_own|multi_model_endpoint_bring_your_own.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-1/advanced_functionality|multi_model_bring_your_own|multi_model_endpoint_bring_your_own.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ca-central-1/advanced_functionality|multi_model_bring_your_own|multi_model_endpoint_bring_your_own.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/sa-east-1/advanced_functionality|multi_model_bring_your_own|multi_model_endpoint_bring_your_own.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-1/advanced_functionality|multi_model_bring_your_own|multi_model_endpoint_bring_your_own.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-2/advanced_functionality|multi_model_bring_your_own|multi_model_endpoint_bring_your_own.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-3/advanced_functionality|multi_model_bring_your_own|multi_model_endpoint_bring_your_own.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-central-1/advanced_functionality|multi_model_bring_your_own|multi_model_endpoint_bring_your_own.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-north-1/advanced_functionality|multi_model_bring_your_own|multi_model_endpoint_bring_your_own.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-1/advanced_functionality|multi_model_bring_your_own|multi_model_endpoint_bring_your_own.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-2/advanced_functionality|multi_model_bring_your_own|multi_model_endpoint_bring_your_own.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-1/advanced_functionality|multi_model_bring_your_own|multi_model_endpoint_bring_your_own.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-2/advanced_functionality|multi_model_bring_your_own|multi_model_endpoint_bring_your_own.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-south-1/advanced_functionality|multi_model_bring_your_own|multi_model_endpoint_bring_your_own.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
